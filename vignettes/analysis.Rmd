---
title: "A real-time calibration method for the numerical pollen forecast model COSMO-ART"
author: "Simon Adamov & Andreas Pauling"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
always_allow_html: TRUE
output:
  html_document:
    df_print: paged 
  pdf_document: default
  word_document: default
---

In this project we want to evaluate the new pollen forecast module that uses "realtime data"
for calibration. As new automatic pollen monitors are being deployed, realtime calibration
of pollen forecast from weather models becomes more and more relevant.

This study uses old measurements and weather model reforcasts to investigate various
implementations and possibilities in terms of forecast improvements.

For very detailed information about the final implementation in COSMO-1E please refer to this 
documentation page (ask meteoswiss for access): https://service.meteoswiss.ch/confluence/x/dYQYBQ

```{r include=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(magrittr)
library(padr)
library(kableExtra)
library(purrr)
library(ggplot2)
library(ggpubr)
library(here)
library(lubridate)
library(caret)
library(psych)
library(scales)
library(nparcomp)

library(conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")

devtools::load_all()
```

```{r include=FALSE}
# Main colors for the analysis
theme_set(theme_minimal(base_size = 14))
col_names <- c("measurement", "baseline", "calibration")
col_hex <- c("#3b3a3a", "#e98962", "#66C2A5")
names(col_hex) <- col_names
```


```{r include=FALSE}
load(paste0(here(), "/data/other/species.RData"))
load(paste0(here(), "/data/other/stations.RData"))
load(paste0(here(), "/data/cosmo/data_cosmo.RData"))
load(paste0(here(), "/data/dwh/data_dwh.RData"))
```

## The Data

Three datasets are available for that endeavour:

- _measurement_ referring to daily pollen concentration measurements as collected by a Hirst trap and available in the Data-Warehouse at meteoswiss
- _baseline_ referring to daily pollen concentration forecasts from the COSMO-1E model *without* calibraton module
- _calibration_ referring to daily pollen concentration forecasts from the COSMO-1E model *with* calibraton module

The following settings are crucial and should always be remembered when running the chunks below:

- The temporal resolution for this analysis is daily averages - pollen verification can be sensitive to temporal resolution
- The species of pollen can be assessed individually or combined 
- The threshold below which Pollen measurements are excluded from the data is set to 10 currently, as they become unreliant

```{r include=FALSE}
data_combined <- data_dwh %>%
  pivot_wider(names_from = type) %>%
  inner_join(data_cosmo %>%
    pivot_wider(names_from = type), by = c("taxon", "station", "date")) %>%
  select(taxon, station, datetime = datetime.x, date, hour = hour.x, measurement, baseline, calibration) %>%
  pivot_longer(measurement:calibration, names_to = "type", values_to = "value") %>%
  select(taxon, station, type, value, datetime, date, hour)

above10_dwh <- data_dwh %>%
  filter(value >= 10) %>%
  select(date, taxon, type, station)

data_above10 <- data_combined %>%
  inner_join(above10_dwh, by = c("date", "taxon", "station")) %>%
  select(taxon, station, type = type.x, value, datetime, date, hour)

data_log10 <- data_above10 %>%
  mutate(value = if_else(value == 0, log10(value + 1), log10(value)))


data_impact_categories <- data_above10 %>%
  pivot_wider(names_from = type) %>%
  mutate(
    categories_measurement = case_when(
      taxon == "Alnus" & measurement < 1 ~ "nothing",
      taxon == "Alnus" & measurement >= 1 & measurement < 10 ~ "weak",
      taxon == "Alnus" & measurement >= 10 & measurement < 70 ~ "medium",
      taxon == "Alnus" & measurement >= 70 & measurement < 250 ~ "strong",
      taxon == "Alnus" & measurement >= 250 ~ "verystrong",
      taxon == "Corylus" & measurement < 1 ~ "nothing",
      taxon == "Corylus" & measurement >= 1 & measurement < 10 ~ "weak",
      taxon == "Corylus" & measurement >= 10 & measurement < 70 ~ "medium",
      taxon == "Corylus" & measurement >= 70 & measurement < 250 ~ "strong",
      taxon == "Corylus" & measurement >= 250 ~ "verystrong",
      taxon == "Betula" & measurement < 1 ~ "nothing",
      taxon == "Betula" & measurement >= 1 & measurement < 10 ~ "weak",
      taxon == "Betula" & measurement >= 10 & measurement < 70 ~ "medium",
      taxon == "Betula" & measurement >= 70 & measurement < 300 ~ "strong",
      taxon == "Betula" & measurement >= 300 ~ "verystrong",
      taxon == "Poaceae" & measurement < 1 ~ "nothing",
      taxon == "Poaceae" & measurement >= 1 & measurement < 20 ~ "weak",
      taxon == "Poaceae" & measurement >= 20 & measurement < 50 ~ "medium",
      taxon == "Poaceae" & measurement >= 50 & measurement < 150 ~ "strong",
      taxon == "Poaceae" & measurement >= 150 ~ "verystrong"
    ),
    categories_baseline = case_when(
      taxon == "Alnus" & baseline < 1 ~ "nothing",
      taxon == "Alnus" & baseline >= 1 & baseline < 10 ~ "weak",
      taxon == "Alnus" & baseline >= 10 & baseline < 70 ~ "medium",
      taxon == "Alnus" & baseline >= 70 & baseline < 250 ~ "strong",
      taxon == "Alnus" & baseline >= 250 ~ "verystrong",
      taxon == "Corylus" & baseline < 1 ~ "nothing",
      taxon == "Corylus" & baseline >= 1 & baseline < 10 ~ "weak",
      taxon == "Corylus" & baseline >= 10 & baseline < 70 ~ "medium",
      taxon == "Corylus" & baseline >= 70 & baseline < 250 ~ "strong",
      taxon == "Corylus" & baseline >= 250 ~ "verystrong",
      taxon == "Betula" & baseline < 1 ~ "nothing",
      taxon == "Betula" & baseline >= 1 & baseline < 10 ~ "weak",
      taxon == "Betula" & baseline >= 10 & baseline < 70 ~ "medium",
      taxon == "Betula" & baseline >= 70 & baseline < 300 ~ "strong",
      taxon == "Betula" & baseline >= 300 ~ "verystrong",
      taxon == "Poaceae" & baseline < 1 ~ "nothing",
      taxon == "Poaceae" & baseline >= 1 & baseline < 20 ~ "weak",
      taxon == "Poaceae" & baseline >= 20 & baseline < 50 ~ "medium",
      taxon == "Poaceae" & baseline >= 50 & baseline < 150 ~ "strong",
      taxon == "Poaceae" & baseline >= 150 ~ "verystrong"
    ),
    categories_calibration = case_when(
      taxon == "Alnus" & calibration < 1 ~ "nothing",
      taxon == "Alnus" & calibration >= 1 & calibration < 10 ~ "weak",
      taxon == "Alnus" & calibration >= 10 & calibration < 70 ~ "medium",
      taxon == "Alnus" & calibration >= 70 & calibration < 250 ~ "strong",
      taxon == "Alnus" & calibration >= 250 ~ "verystrong",
      taxon == "Corylus" & calibration < 1 ~ "nothing",
      taxon == "Corylus" & calibration >= 1 & calibration < 10 ~ "weak",
      taxon == "Corylus" & calibration >= 10 & calibration < 70 ~ "medium",
      taxon == "Corylus" & calibration >= 70 & calibration < 250 ~ "strong",
      taxon == "Corylus" & calibration >= 250 ~ "verystrong",
      taxon == "Betula" & calibration < 1 ~ "nothing",
      taxon == "Betula" & calibration >= 1 & calibration < 10 ~ "weak",
      taxon == "Betula" & calibration >= 10 & calibration < 70 ~ "medium",
      taxon == "Betula" & calibration >= 70 & calibration < 300 ~ "strong",
      taxon == "Betula" & calibration >= 300 ~ "verystrong",
      taxon == "Poaceae" & calibration < 1 ~ "nothing",
      taxon == "Poaceae" & calibration >= 1 & calibration < 20 ~ "weak",
      taxon == "Poaceae" & calibration >= 20 & calibration < 50 ~ "medium",
      taxon == "Poaceae" & calibration >= 50 & calibration < 150 ~ "strong",
      taxon == "Poaceae" & calibration >= 150 ~ "verystrong"
    )
  ) %>%
  mutate_at(
    vars(categories_measurement, categories_baseline, categories_calibration),
    ~ factor(., levels = c("nothing", "weak", "medium", "strong", "verystrong"))
  ) %>%
  filter(categories_measurement != "nothing")


data_impact_categories_mean <- data_above10 %>%
  pivot_wider(names_from = type, values_from = value) %>%
  mutate(mean = (measurement + baseline + calibration) / 3,
    categories_mean = case_when(
      taxon == "Alnus" & mean < 1 ~ "nothing",
      taxon == "Alnus" & mean >= 1 & mean < 10 ~ "weak",
      taxon == "Alnus" & mean >= 10 & mean < 70 ~ "medium",
      taxon == "Alnus" & mean >= 70 & mean < 250 ~ "strong",
      taxon == "Alnus" & mean >= 250 ~ "verystrong",
      taxon == "Corylus" & mean < 1 ~ "nothing",
      taxon == "Corylus" & mean >= 1 & mean < 10 ~ "weak",
      taxon == "Corylus" & mean >= 10 & mean < 70 ~ "medium",
      taxon == "Corylus" & mean >= 70 & mean < 250 ~ "strong",
      taxon == "Corylus" & mean >= 250 ~ "verystrong",
      taxon == "Betula" & mean < 1 ~ "nothing",
      taxon == "Betula" & mean >= 1 & mean < 10 ~ "weak",
      taxon == "Betula" & mean >= 10 & mean < 70 ~ "medium",
      taxon == "Betula" & mean >= 70 & mean < 300 ~ "strong",
      taxon == "Betula" & mean >= 300 ~ "verystrong",
      taxon == "Poaceae" & mean < 1 ~ "nothing",
      taxon == "Poaceae" & mean >= 1 & mean < 20 ~ "weak",
      taxon == "Poaceae" & mean >= 20 & mean < 50 ~ "medium",
      taxon == "Poaceae" & mean >= 50 & mean < 150 ~ "strong",
      taxon == "Poaceae" & mean >= 150 ~ "verystrong"
  )) %>%
  pivot_longer(measurement:calibration, names_to = "type", values_to = "value") %>%
  pivot_wider(names_from = categories_mean, values_from = value)


```

## Visual Assessment

### Basic Plots

In the following the user can select one species and one year to compare the three datasets.

```{r include=FALSE}
data_timeseries <-
  data_combined %>%
  filter(
    between(date, as.Date("2020-01-15"), as.Date("2020-04-01")),
    station == "Zürich",
    taxon == "Alnus"
  )

gg1 <- data_timeseries %>%
  ggplot(aes(x = datetime)) +
  geom_line(aes(y = value, col = type), alpha = 0.8) +
  labs(y = "Mean Conc. [Pollen/m³]", x = "") +
  theme(legend.position = "none") +
  scale_color_manual(values = col_hex)
```


```{r include=FALSE}
gg2 <- data_timeseries %>%
  ggplot() +
  geom_boxplot(aes(y = log10(value), fill = type)) +
  theme(
    legend.position = "none",
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank()
  ) +
  labs(y = "Log Mean Conc. [Pollen/m³]", x = "") +
  scale_fill_manual(values = col_hex)
```

```{r include=FALSE}

sd_hirst <- data_timeseries %>%
  group_by(type) %>%
  summarise(sd = sd(value, na.rm = TRUE))

(gg3 <- data_timeseries %>%
  ggplot() +
  geom_histogram(aes(
    y = log10(value),
    fill = type
  ),
  binwidth = 0.1
  ) +
  geom_label(
    data = sd_hirst,
    aes(
      label = paste("Standard Deviation:\n", round(sd), "Pollen / m³"),
      x = 8,
      y = 2,
      group = type
    ),
    size = 3
  ) +
  facet_wrap(vars(type), ncol = 1) +
  theme(legend.position = "bottom") +
  scale_fill_manual(values = col_hex) +
  coord_flip() +
  labs(x = "Occurence of Pollen Concentrations", y = "Log Mean Conc. [Pollen/m³]"))

```

```{r echo=FALSE, fig.height = 8, fig.width = 13, fig.dpi=300, out.width="100%"}
ggarrange(ggarrange(gg1, gg2, nrow = 2), gg3) %>%
  annotate_figure(top = "Comparison of Models for one Station in one Year")
```

### Correlation Plots

The correlation between the Model and Measurements can be calculated easily and then the CI and p-values must be adjusted for multiple comparison.
The corr-test function from the psych handily offers this functionality.

Careful the correlation coefficients method have some serious shortcomings:

The correlation coefficient measures linear agreement--whether the measurements go up-and-down together. Certainly, we want the measures to go up-and-down together, but the correlation coefficient itself is deficient in at least three ways as a measure of agreement. (http://www.jerrydallal.com/LHSP/compare.htm)

- The correlation coefficient can be close to 1 (or equal to 1!) even when there is considerable bias between the two methods. For example, if one method gives measurements that are always 10 units higher than the other method, the correlation will be 1 exactly, but the measurements will always be 10 units apart.
- The magnitude of the correlation coefficient is affected by the range of subjects/units studied. The correlation coefficient can be made smaller by measuring samples that are similar to each other and larger by measuring samples that are very different from each other. The magnitude of the correlation says nothing about the magnitude of the differences between the paired measurements which, when you get right down to it, is all that really matters.
- The usual significance test involving a correlation coefficient-- whether the population value is 0--is irrelevant to the comparability problem. What is important is not merely that the correlation coefficient be different from 0. Rather, it should be close to (ideally, equal to) 1! 

A good summary of the methods and their shortcomings can be found here: https://www.statisticssolutions.com/correlation-Pearson-Kendall-spearman/

```{r include=FALSE}

methods <- c("pearson", "spearman", "kendall")

# For the robust methods (spearman, kendall)
# it doesn't matter whether the transformed data is used or the original

data_corr <- data_log10 %>%
  pivot_wider(names_from = type) %>%
  select(datetime, measurement, baseline, calibration)

corr_matrix <- map(methods, ~ corr.test(
  data_corr %>% select(-datetime),
  use = "complete",
  method = .x,
  adjust = "holm",
  alpha = .05,
  ci = TRUE,
  minlength = 5
))
```

```{r include=FALSE}
ci <- map(corr_matrix, ~ .x %>%
  pluck(10)) %>%
  bind_rows() %>%
  round(2) %>%
  mutate(
    method = rep(methods, each = 3),
    metric = rep(c("R-", "rho-", "tau-"), each = 3),
    label = tools::toTitleCase(paste0(metric, method, ": ", lower, " - ", upper)),
    comparison = rep(c("mb", "mc", "bc"), times = 3),
    x = rep(c(0.6, 0.64, 0.55), each = 3),
    y = rep(c(3.2, 3.4, 3.6), each = 3)
  )

```

```{r include=FALSE}
gg_corr1 <- data_corr %>%
  ggplot(aes(x = measurement, y = baseline)) +
  geom_point(alpha = 0.1, col = "#222225") +
  geom_smooth(data = data_corr, alpha = 0.3, col = "#3081b8", fill = "#74cbee") +
  geom_abline(slope = 1, intercept = 0, col = "#cc2d2d") +
  geom_label(data = ci %>% filter(comparison == "mb"), aes(label = label, x = x, y = y), parse = TRUE) +
  coord_cartesian(ylim = c(0, 4), xlim = c(0, 4))
```

```{r include=FALSE}
gg_corr2 <- data_corr %>%
  ggplot(aes(x = measurement, y = calibration)) +
  geom_point(alpha = 0.1, col = "#222225") +
  geom_smooth(data = data_corr, alpha = 0.3, col = "#3081b8", fill = "#74cbee") +
  geom_abline(slope = 1, intercept = 0, col = "#cc2d2d") +
  geom_label(data = ci %>% filter(comparison == "mc"), aes(label = label, x = x, y = y), parse = TRUE) +
  coord_cartesian(ylim = c(0, 4), xlim = c(0, 4))
```


```{r echo=FALSE, fig.height = 8, fig.width = 13, fig.dpi=300, out.width="100%"}
(gg_corr <- ggarrange(gg_corr1, gg_corr2, ncol = 2) %>%
  annotate_figure(
    top = "Pairwise correlation between Models",
    bottom = text_grob("Pairwise correlation between Models; blue line shows the Loess smoother; the red line shows a theroratical perfect correlation of 1. \n In the text box one can see the 95% confidence intervals of the R-values (adjusted for multiple comparison) as obtained by Pearson and two robust methods.",
      face = "italic", size = 10
    )
  ))
```

## Altman Bland Plots

The well established AB-method for clinical trials can be used here as well to compare the means and differences between datasets. 
If the points lie within the two SD-line for the differences the datasets can be assumed to be strongly associated with each other. 

```{r include=FALSE}

data_altman <- data_above10 %>%
  pivot_wider(names_from = type) %>%
  select(datetime, measurement, baseline, calibration) %>%
  mutate(
    mean_baseline = (measurement + baseline) / 2,
    mean_calibration = (measurement + calibration) / 2,
    diff_baseline = baseline - measurement,
    diff_calibration = calibration - measurement
  )


sd_diff <- data_altman %>%
  select(starts_with("diff")) %>%
  summarise_all(~ sd(.)) %>%
  pivot_longer(1:2, values_to = "sd", names_to = "dummy") %>%
  pull(sd)

gg_ab1 <- data_altman %>%
  ggplot(aes(x = mean_baseline, y = diff_baseline)) +
  geom_point(alpha = 0.1, col = "#222225") +
  coord_cartesian(
    xlim = c(0, 1000),
    ylim = c(
      -sd_diff[1] * 4,
      sd_diff[1] * 4
    )
  ) +
  geom_abline(
    slope = 0, intercept = 0, alpha = 0.8, col = "#cc2d2d"
  ) +
  geom_abline(
    slope = 0, col = "#cc2d2d",
    intercept = sd_diff[1] * 2, alpha = 0.8, linetype = 3
  ) +
  geom_abline(
    slope = 0, col = "#cc2d2d",
    intercept = sd_diff[1] * (-2), alpha = 0.8, linetype = 3
  ) +
  geom_smooth(alpha = 0.3, col = "#3081b8", fill = "#74cbee") +
  labs(y = "Difference(Baseline - Measurement)", x = "Mean(Baseline, Measurement)")
```

```{r include=FALSE}

gg_ab2 <- data_altman %>%
  ggplot(aes(x = mean_calibration, y = diff_calibration)) +
  geom_point(alpha = 0.1, col = "#222225") +
  coord_cartesian(
    xlim = c(0, 1000),
    ylim = c(
      -sd_diff[2] * 4,
      sd_diff[2] * 4
    )
  ) +
  geom_abline(
    slope = 0, intercept = 0, alpha = 0.8, col = "#cc2d2d"
  ) +
  geom_abline(
    slope = 0, col = "#cc2d2d",
    intercept = sd_diff[2] * 2, alpha = 0.8, linetype = 3
  ) +
  geom_abline(
    slope = 0, col = "#cc2d2d",
    intercept = sd_diff[2] * (-2), alpha = 0.8, linetype = 3
  ) +
  geom_smooth(alpha = 0.3, col = "#3081b8", fill = "#74cbee") +
  labs(y = "Difference(Calibration - Measurement)", x = "Mean(Calibration, Measurement)")
```

```{r echo=FALSE, fig.height = 8, fig.width = 13, fig.dpi=300, out.width="100%"}
(gg_ab <- ggarrange(gg_ab1, gg_ab2, ncol = 2) %>%
  annotate_figure(top = "Altman-Bland Plots", bottom = text_grob("Pairwise comparison of Models; blue line shows the Loess smother; the red line shows a theroratical perfect
  agreement between Model and Measurements of zero. The dashed red line shows the 2 * sd of the differences, where we expect the points to lie within.",
    face = "italic", size = 12
  )))

```

## Density Plots

These plots allow to observe the error for different concentration categories.

```{r include=FALSE}
categs <- c("weak", "medium", "strong", "verystrong")

gg_conc_dens <- list()

labels_y <- list(0.1, 0.015, 0.004, 0.002)
labels_y_hist <- list(15, 13, 10, 10, 7.5, 3)
names(labels_y) <- categs

for (j in categs) {
  if (j %in% names(data_impact_categories_mean)) {
    obs <- data_impact_categories_mean %>%
      filter(!is.na(!!sym(j))) %>%
      summarise(n() / 2) %>%
      pull()
    obs <- paste("# of Observations:", obs)

    gg_conc_dens[[j]] <- data_impact_categories_mean %>%
      filter(!is.na(!!sym(j))) %>%
      ggplot() +
      # The area under that whole curve should be 1.
      # To get an estimate of the probability of certain values,
      # you'd have to integrate over an interval on your 'y' axis,
      # and that value should never be greater than 1.
      geom_density(aes(x = !!sym(j), col = type, fill = type), alpha = 0.15) +
      geom_label(label = obs, aes(x = max(!!sym(j)) * 0.7), y = labels_y[[j]]) +
      scale_colour_manual("", values = col_hex) +
      scale_fill_manual(values = col_hex) +
      coord_cartesian(xlim = c(0, NA)) +
      guides(fill = "none")
  }
}

```

```{r echo=FALSE, fig.height = 8, fig.width = 13, fig.dpi=300, out.width="100%"}

(gg_dens_conc <- ggarrange(plotlist = gg_conc_dens) %>%
  annotate_figure(
    top = paste(
      "Comparison of Measurements and Model Predictions",
      "for All Stations and Different Concentration Groups."
    ),
    bottom = text_grob(paste0("We are looking at Density Kernel Estimators ",
      "for all three traps to compare the measurements between them. ",
      "\n The area under each curve adds up to 1 and makes it possible ",
      "to vizualise the (dis-)similarities of measurements from the ",
      "three traps. \n It is basically a smoothed histogram. ",
      "The buckets are based on the mean concentrations of measurements and model."),
      face = "italic",
      size = 10
    )
  ))

```


## Statistical Assessment

First, various metrics are compared where the pollen concentrations are considered a continuous numerical variable.

```{r echo=FALSE}
metrics_baseline <- data_above10 %>%
  pivot_wider(names_from = type) %>%
  filter(!is.na(baseline)) %>%
  mutate(
    error = baseline - measurement,
  ) %>%
  summarise(
    R2 = cor(baseline, measurement, method = "spearman", use = "complete.obs")^2,
    Bias = mean(error),
    SD = sd(error),
    MAE = mean(abs(error)),
    RMSE = sqrt(mean((error)^2)),
    MSLE = mean((log(1 + baseline) - log(1 + measurement))^2, na.rm = TRUE),
    RMSLE = sqrt(MSLE)
  ) %>%
  mutate(type = "baseline")

metrics_calibration <- data_above10 %>%
  pivot_wider(names_from = type) %>%
  filter(!is.na(calibration)) %>%
  mutate(
    error = calibration - measurement,
  ) %>%
  summarise(
    R2 = cor(calibration, measurement, method = "spearman", use = "complete.obs")^2,
    Bias = mean(error),
    SD = sd(error),
    MAE = mean(abs(error)),
    RMSE = sqrt(mean((error)^2)),
    MSLE = mean((log(1 + calibration) - log(1 + measurement))^2, na.rm = TRUE),
    RMSLE = sqrt(MSLE)
  ) %>%
  mutate(type = "calibration")

metrics_baseline %>%
  bind_rows(metrics_calibration) %>%
  kable() %>%
  kable_styling("striped", full_width = FALSE)
```
Second, the values will be converted into health impact based buckets.
The impact classes have been defined https://service.meteoswiss.ch/confluence/x/1ZG4
Now we can investigate various metrics that are typically used for categoric variables.
The Kappa metric is explained here and was chosen as the most meaningful metric for this analysis:
https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c

```{r echo=FALSE}
matrix_baseline <- confusionMatrix(
  data_impact_categories$categories_baseline,
  data_impact_categories$categories_measurement
)
kappa_baseline <- matrix_baseline$overall[1:2] %>%
  tibble() %>%
  mutate(
    type = "baseline",
    metric = c("Accuracy", "Kappa")
  )

matrix_calibration <- confusionMatrix(
  data_impact_categories$categories_calibration,
  data_impact_categories$categories_measurement
)
kappa_calibration <- matrix_calibration$overall[1:2] %>%
  tibble() %>%
  mutate(
    type = "calibration",
    metric = c("Accuracy", "Kappa")
  )

kappa_baseline %>%
  bind_rows(kappa_calibration) %>%
  pivot_wider(names_from = metric, values_from = ".") %>%
  kable() %>%
  kable_styling("striped", full_width = FALSE)
```

The following table could be used in the appendix.

Reference Event No Event
Predicted 
Event     A        B
No Event  C        D
The formulas used here are:

- Sensitivity = A/(A+C)
- Specificity = D/(B+D)
- Prevalence = (A+C)/(A+B+C+D)
- PPV = (sensitivity * prevalence)/((sensitivity*prevalence) + ((1-specificity)*(1-prevalence)))
- NPV = (specificity * (1-prevalence))/(((1-sensitivity)*prevalence) + ((specificity)*(1-prevalence)))
- Detection Rate = A/(A+B+C+D)
- Detection Prevalence = (A+B)/(A+B+C+D)
- Balanced Accuracy = (sensitivity+specificity)/2
- Precision = A/(A+B)
- Recall = A/(A+C)
- F1 = (1+beta^2)*precision*recall/((beta^2 * precision)+recall)

```{r echo=FALSE}
matrix_baseline$byClass %>%
  as_tibble() %>%
  mutate(
    class = rownames(matrix_baseline$byClass),
    type = "baseline"
  ) %>%
  filter(class != "Class: nothing") %>%
  bind_rows(
    matrix_calibration$byClass %>%
      as_tibble() %>%
      mutate(
        class = rownames(matrix_calibration$byClass),
        type = "calibration"
      ) %>%
      filter(class != "Class: nothing")
  ) %>%
  mutate(across(c(-class, -type), ~ round(., digits = 3))) %>%
  kable() %>%
  kable_styling("striped", full_width = FALSE)
```

## Robust Contrasts with Confidence Intervals

https://www.researchgate.net/publication/282206980_nparcomp_An_R_Software_Package_for_Nonparametric_Multiple_Comparisons_and_Simultaneous_Confidence_Intervals 
The R package nparcomp implements a broad range of rank-based nonparametric methods for multiple comparisons. 
The single step procedures provide local test decisions in terms of multiplicity adjusted p-values and simultaneous conﬁdence intervals. 
The null hypothesis H0: p = 1/2 is significantly rejected at 5% level of significance for many pairwise comparisons.
Whenever the p-Value is < than 5% = the confidence interval contains 0.5 -> the effect from the factor trap is not statistically meaningful.
The Estimator can also be interpreted as a proxy for the relative difference in median between Model and Measurements.
If the Estimator is > 0.5 then the second trap tends to have larger measurements.

```{r echo=FALSE}

npar_contr <- map(species$taxon[-3], ~
  nparcomp(
    value ~ type,
    data = data_log10 %>%
      filter(taxon == .x),
    # slice(1:3870),
    conf.level = 0.95,
    alternative = "two.sided",
    type = "Dunnet",
    control = "measurement"
  ))

title <- paste(
  "Robust Contrasts and Confidence Intervals for Corylus Measurements"
)
myheader <- c(title = 6)
names(myheader) <- title

npar_contr[[1]]$Analysis %>%
  bind_rows(
    npar_contr[[2]]$Analysis,
    npar_contr[[2]]$Analysis
  ) %>%
  mutate(taxon = rep(species$taxon[-3], each = 2)) %>%
  mutate(across(where(is.numeric), round, 3)) %>%
  select(Taxon = taxon, Comparison, Estimator, Lower, Upper, pValue = p.Value) %>%
  mutate(pValue = if_else(pValue < 0.05,
    cell_spec(pValue, color = "red"),
    cell_spec(pValue)
  )) %>%
  kable(escape = FALSE) %>%
  kable_styling("striped", full_width = FALSE) %>%
  add_header_above(myheader)
```